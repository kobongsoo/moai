{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820aa697-ef0d-42e5-8f0f-04852a14e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# 문서들을 embed하는 예제-3\n",
    "# - 여기서는 문서들을 전처리하고, 임베딩 하는 과정임.\n",
    "#\n",
    "# 질의 응답 시스템 과정\n",
    "# 문서들 전처리 : \n",
    "#    1.문서에서 문장들 나눔-문장들을 LEN 만큼 나눠서 임시 chunk 생성\n",
    "#    - USE_GPT_PREPROCESSING=False : chunk 그대로 사용\n",
    "#    - USE_GPT_PREPROCESSING=True : chunk를 GPT에 입력해서 chunk 내용 정리함.\n",
    "#    2. 불용어 제거 -문장별루 분할.\n",
    "#    3. 임베딩 설정값에 따라 ES에 chunk 임베딩\n",
    "# 임베딩 : \n",
    "#    kpf-sbert-v1.1로  문장 평균 임베딩벡터 구함 - es에 문장별루 단락text와 평균벡터 저장.\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "import openai  \n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict, List, Optional\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import MyUtils\n",
    "from utils import get_sentences, generate_text_GPT2, generate_text_davinci\n",
    "\n",
    "myutils = MyUtils(yam_file_path='../data/settings.yaml')\n",
    "settings = myutils.get_options()\n",
    "\n",
    "myutils.seed_everything()  # seed 설정\n",
    "DEVICE = myutils.GPU_info() # GPU 혹은 CPU\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DATA_FOLDER = '../doc/company_2024/'  # text 추출된 파일이 있는 폴더\n",
    "SPLIT_SENTENCE_LEN = 30 # chunk만들 문장 계수 = doc를 x계수만큼씩 문장으로 나눠서 chunk 만듬.\n",
    "\n",
    "# gpt 정보 설정-----------------------------------------------------------------\n",
    "USE_GPT_PREPROCESSING = True  # True= GPT로 전처리 수행 함, False=전처리 수행 안함.\n",
    "\n",
    "# USE_GPT_PREPROCESSING=True일때 적용됨\n",
    "GPT_MODEL:str = settings['GPT_MODEL']   # GPT 모델 종류 \n",
    "MAX_TOKENS = 4096 #토큰 수  \n",
    "TEMPERATURE = 1.0 # temperature 0~2 범위 : 작을수록 정형화된 답변, 클수록 유연한 답변(2는 엉뚱한 답변을 하므로, 1.5정도가 좋은것 같음=기본값은=1)\n",
    "TOP_P = 0.2       #기본값은 1 (0.1이라고 하면 10% 토큰들에서 출력 토큰들을 선택한다는 의미)\n",
    "STREAM = False    # 스트림으로 출력할지(실시간)\n",
    "\n",
    "openai.api_key = settings['GPT_TOKEN']# **GPT  key 지정\n",
    "#PROMPT = 'Q:위 내용들을 잘 정리해 주세요.'\n",
    "PROMPT = 'Q:위내용에서 주요 주제를 파악하고 그에 따라 문장들을 구조화해주세요. 또한 문장에서 나온 사실적 정보들을 정리하여 제시해주세요'\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# ES 임베딩 설정---------------------------------------------------------------------\n",
    "OUT_DIMENSION = 128   # 128 혹은 768이면 0입력\n",
    "EMBEDDING_METHOD=0   # 0=클러스터링 임베딩, 1=문장평균임베딩, 2=문장임베딩\n",
    "NUM_CLUSTERS=10      # 클러스터링 임베딩일때 클러스터링 수 \n",
    "NUM_CLUSTERS_VARIABLE=False # 클러스터링 임베딩일때 클러스터링을 문장계수마다 다르계할지.\n",
    "CLUSTRING_MODE = \"kmeans\"  # \"kmeans\" = k-평균 군집 분석, kmedoids =  k-대표값 군집 분석\n",
    "OUTMODE = \"mean\"           # kmeans 일때=>mean=평균임베딩값, max=최대임베딩값.,  kmedoids 일때 =>mean=평균임베딩값, medoid=대표임베딩값.\n",
    "\n",
    "MODEL_PATH = '../model/kpf-sbert-128d-v1'#'../../../data11/model/kpf-sbert-v1.1'\n",
    "POLLING_MODE = 'mean' # 폴링모드 \n",
    "\n",
    "FLOAT_TYPE = 'float16' # float32 혹은 float16\n",
    "SEED = 111\n",
    "\n",
    "# ES 접속\n",
    "ES_URL = 'http://192.168.0.51:9200/'             # es 접속 주소\n",
    "ES_INDEX_NAME = 'qaindex_128_10_sentence_30_gpt_1'     # 생성혹은 추가할 인덱스 명\n",
    "ES_INDEX_FILE = '../data/mpower10u_128d_10.json'  # 인덱스 구조 파일경로\n",
    "BATCH_SIZE=20       # ES 배치 사이즈\n",
    "CREATE_INDEX = True # True이면 기존에 인덱스가 있다면 제거하고 다시 생성.\n",
    "#------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e33b1-6cbf-4fc8-ae82-50577909b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER에 파일들을 불러와서 DF로 만듬.\n",
    "# -문서는 MAX_LEN 길이만큼씩 잘라서 만듬.\n",
    "\n",
    "# 파일이 여러개인 경우 폴더 지정\n",
    "files = myutils.getListOfFiles(DATA_FOLDER)\n",
    "assert len(files) > 0 # files가 0이면 assert 발생\n",
    "print('*file_count: {}, file_list:{}'.format(len(files), files[0:5]))\n",
    "\n",
    "# 파일 경로를 지정하면됨.\n",
    "#files = [\"../data11/mpower_doc/신입사원교육.txt\",]\n",
    "docid = 0  # **카운터가 문서에 uid가 되므로, 유일무이한 값므로 지정할것.\n",
    "count = 0\n",
    "\n",
    "titles = []\n",
    "contextids = []\n",
    "contexts = []\n",
    "\n",
    "for idx, file_path in enumerate(files):\n",
    "    if '.ipynb_checkpoints' not in file_path:\n",
    "        docid += 1\n",
    "        count = 0\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "            # 파일명만 추출하여 titles에 저장\n",
    "            filename, ext = os.path.splitext(os.path.basename(file_path))  # 파일명과 확장자(.txt, .doc 등) 분리\n",
    "            titles.append(filename)\n",
    "            contextids.append(f'{docid}_{count}')\n",
    "            contexts.append(data.strip()) # 두번째는 문서 제목+문서내용 합처서 문장만듬.\n",
    "            \n",
    "# 데이터 프레임으로 만듬.\n",
    "df_contexts = pd.DataFrame((zip(contexts, titles, contextids)), columns = ['context','question', 'contextid'])     \n",
    "\n",
    "print(f'*doc 총 계수:{len(df_contexts)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef55ea-bd94-4d73-90b3-9f1b7916e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일로 저장해둠.\n",
    "import json\n",
    "examples = []\n",
    "\n",
    "for context, title, contextid in zip(contexts, titles, contextids):\n",
    "    doc = {}\n",
    "    doc['context'] = context  # 문단\n",
    "    doc['title'] = title      # 제목\n",
    "    doc['contextid'] = contextid # 문서 id\n",
    "                        \n",
    "    examples.append(doc)\n",
    "                        \n",
    "docs ={}\n",
    "docs['text'] = examples\n",
    "\n",
    "#json 파일로 저장.\n",
    "# JSON 파일을 엽니다.\n",
    "with open(\"../doc/docs.json\", \"w\") as f:\n",
    "    # 리스트를 JSON으로 변환합니다.\n",
    "    json.dump(docs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c584631-299d-4cee-9873-a4fa0ad367fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장들로 분리\n",
    "# => 이때는 chunk를 만들기 위한것이므로, remove_sentence_len=0, remove_duplication=False, remove_prefix_pattern= False로 지정함\n",
    "doc_sentences = get_sentences(df=df_contexts, remove_sentence_len=2, remove_duplication=False, remove_prefix_pattern=False)\n",
    "\n",
    "#print(f'len:{len(doc_sentences)}, 1: {doc_sentences[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f8608-a4bc-453b-9dbf-a63331fdf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT로 요약본 생성\n",
    "def generate(settings:dict, chunk:str):\n",
    "\n",
    "    input_prompt = chunk+'\\n\\n'+ PROMPT\n",
    "    if GPT_MODEL.startswith(\"gpt-\"):\n",
    "        response, status = generate_text_GPT2(gpt_model=GPT_MODEL, prompt=input_prompt, system_prompt=\"\", \n",
    "                                              assistants=[], stream=STREAM, timeout=20,\n",
    "                                              max_tokens=MAX_TOKENS, temperature=TEMPERATURE, top_p=TOP_P) \n",
    "    else:\n",
    "        response, status = generate_text_davinci(gpt_model=GPT_MODEL, prompt=input_prompt, stream=stream, timeout=20,\n",
    "                                                 max_tokens=MAX_TOKENS, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "\n",
    "    return response, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99ab7c-50d0-48c0-a9cf-854e28a6e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분리된 문장들을 SPLIT_SENTENCE_LEN 계수만큼씩 분할해서 chunk 만듬\n",
    "docs_split=[]\n",
    "for count, doc in enumerate(doc_sentences):\n",
    "    temp = []\n",
    "    for i in range(0, len(doc), SPLIT_SENTENCE_LEN):\n",
    "        chunk = doc[i:i+SPLIT_SENTENCE_LEN]\n",
    "        if chunk:\n",
    "            temp.append(chunk)\n",
    "    if len(temp) > 0 and len(temp[-1]) < SPLIT_SENTENCE_LEN:\n",
    "        remaining = temp.pop(-1)\n",
    "\n",
    "        if temp:\n",
    "            temp[-1].extend(remaining)\n",
    "        else:\n",
    "            temp.append(remaining)\n",
    "    docs_split.append(temp)\n",
    "\n",
    "#print(docs_split)\n",
    "\n",
    "# X 씩 분할된 chunk에 title, contextid등을 붙임.\n",
    "titles = []\n",
    "contextids = []\n",
    "contexts = []\n",
    "org_contexts = []\n",
    "\n",
    "for idx, docs in enumerate(tqdm(docs_split)):\n",
    "    #print(idx)\n",
    "    #paragraph_list = []\n",
    "    count = 0\n",
    "    for paragraph in docs:\n",
    "        chunk_tmp = ''\n",
    "        for sentence in paragraph:\n",
    "            chunk_tmp +='\\n'+sentence\n",
    "\n",
    "        if chunk_tmp:\n",
    "            title = df_contexts['question'][idx]\n",
    "            titles.append(title)\n",
    "            contextids.append(f'{idx}_{count}')\n",
    "            org_contexts.append(title+'\\n\\n'+chunk_tmp.strip())    \n",
    "            \n",
    "            if USE_GPT_PREPROCESSING == True:\n",
    "                # GPT에 입력해서 chunk_tmp 입력해서 정리함.\n",
    "                chunk = title+'\\n\\n'+chunk_tmp.strip()\n",
    "                response, status = generate(settings=settings, chunk=chunk)\n",
    "                if count < 3:  # 문서당 3개만 출력함\n",
    "                    print(f'\\nin(status):{status}:\\n{chunk}\\n')\n",
    "                    print(f'\\nout:\\n{response}')\n",
    "\n",
    "                if status == 0:\n",
    "                    contexts.append(title+'\\n\\n'+response.strip()) \n",
    "                else:\n",
    "                    print(f'\\ngpt error:\\n{response}')\n",
    "                    contexts.append(title+'\\n\\n'+chunk_tmp.strip()) #에러나면 원본을  context에 담음\n",
    "            else:\n",
    "                #paragraph_list.append(tmp[1:]) # 맨앞 공백은 제거\n",
    "                contexts.append(title+'\\n\\n'+chunk_tmp.strip())    \n",
    "                \n",
    "            count +=1\n",
    "\n",
    "# 데이터 프레임으로 만듬.\n",
    "df_contexts = pd.DataFrame((zip(contexts, org_contexts, titles, contextids)), columns = ['context', 'org_context', 'question', 'contextid'])     \n",
    "print(f'\\r\\n*chunk 총 계수:{len(df_contexts)}\\n')\n",
    "\n",
    "# JSON 파일로 저장해둠.(옵션)\n",
    "import json\n",
    "examples = []\n",
    "\n",
    "for org_contexts, context, title, contextid in zip(org_contexts, contexts, titles, contextids):\n",
    "    doc = {}\n",
    "    doc['org_context'] = org_contexts        # 문단 원본 내용\n",
    "    doc['context'] = context  # 문단내용(*GPT 요약인 경우 문단 org 내용과 다를수 있음.)\n",
    "    doc['title'] = title      # 제목\n",
    "    doc['contextid'] = contextid # 문서 id\n",
    "                        \n",
    "    examples.append(doc)\n",
    "                        \n",
    "docs ={}\n",
    "docs['text'] = examples\n",
    "\n",
    "#json 파일로 저장.\n",
    "with open(\"../doc/chunk.json\", \"w\") as f:\n",
    "    # 리스트를 JSON으로 변환합니다.\n",
    "    json.dump(docs, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4019de-b616-44e3-9ba7-226547bfab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 저장된 chunk.json 파일을 불러옴.\n",
    "# JSON 파일을 읽어옵니다.\n",
    "with open(\"../doc/chunk.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 데이터 프레임으로 변환합니다.\n",
    "df_contexts = pd.DataFrame(data['text'])\n",
    "\n",
    "# 데이터프레임을 출력합니다.\n",
    "#print(df_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5bac1-60e7-4680-bde0-8900fa072069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contexts['context'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78dddb2-f820-493d-b9ae-e307a8966f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 하기 위해 chunk를 문장들로 분리\n",
    "# => 이때는 remove_sentence_len=3로 해서 3문자 이하는 제거 함.\n",
    "doc_sentences = get_sentences(df=df_contexts, remove_sentence_len=3, remove_duplication=False)\n",
    "\n",
    "#print(f'len:{len(doc_sentences)}, 1: {doc_sentences[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48638f8-0fbf-4c38-92c0-46adf2a51916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 추가\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import embed_text, bi_encoder, mpower_index_batch, create_index, clustering_embedding, kmedoids_clustering_embedding\n",
    "\n",
    "# ES 관련\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "    \n",
    "# 조건에 맞게 임베딩 처리하는 함수 \n",
    "def embedding(paragraphs:list)->list:\n",
    "    # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "    embeddings = embed_text(model=BI_ENCODER1, paragraphs=paragraphs, return_tensor=False).astype(FLOAT_TYPE)    \n",
    "    return embeddings\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#문단에 문장들의 임베딩을 구하여 각각 클러스터링 처리함.\n",
    "#---------------------------------------------------------------------------\n",
    "def index_data(es, df_contexts, doc_sentences:list):\n",
    "    #클러스터링 계수는 문단의 계수보다는 커야 함. \n",
    "    #assert num_clusters <= len(doc_sentences), f\"num_clusters:{num_clusters} > len(doc_sentences):{len(doc_sentences)}\"\n",
    "    #-------------------------------------------------------------\n",
    "    # 각 문단의 문장들에 벡터를 구하고 리스트에 저장해 둠.\n",
    "    start = time.time()\n",
    "    cluster_list = []\n",
    "\n",
    "    rfile_names = df_contexts['contextid'].values.tolist()\n",
    "    rfile_texts = df_contexts['context'].values.tolist()\n",
    "\n",
    "    if OUT_DIMENSION == 0:\n",
    "        dimension = 768\n",
    "    else:\n",
    "        dimension = 128\n",
    "\n",
    "    clustering_num = NUM_CLUSTERS\n",
    "        \n",
    "    docs = []\n",
    "    count = 0\n",
    "    for i, sentences in enumerate(tqdm(doc_sentences)):\n",
    "        embeddings = embedding(sentences)\n",
    "        if i < 3:\n",
    "            print(f'[{i}] sentences-------------------')\n",
    "            if len(sentences) > 5:\n",
    "                print(sentences[:5])\n",
    "            else:\n",
    "                print(sentences)\n",
    "                \n",
    "            myutils.log_message(f'*[index_data] embeddings.shape: {embeddings.shape}', log_folder='../log/documet')\n",
    "            print()\n",
    "        \n",
    "        #----------------------------------------------------------------\n",
    "        multiple = 1\n",
    "        \n",
    "        # [bong][2023-04-28] 임베딩 출력 계수에 따라 클러스터링 계수를 달리함.\n",
    "        if NUM_CLUSTERS_VARIABLE == True:\n",
    "            embeddings_len = embeddings.shape[0]\n",
    "            if embeddings_len > 2000:\n",
    "                multiple = 6\n",
    "            elif embeddings_len > 1000:\n",
    "                multiple = 5 # 5배\n",
    "            elif embeddings_len > 600:\n",
    "                multiple = 4 # 4배\n",
    "            elif embeddings_len > 300:\n",
    "                multiple = 3 # 3배\n",
    "            elif embeddings_len > 100:\n",
    "                multiple = 2 # 2배\n",
    "        #----------------------------------------------------------------\n",
    "        \n",
    "        # 0=문장클러스터링 임베딩\n",
    "        if EMBEDDING_METHOD == 0:\n",
    "            if CLUSTRING_MODE == \"kmeans\":\n",
    "                # 각 문단에 분할한 문장들의 임베딩 값을 입력해서 클러스터링 하고 평균값을 구함.\n",
    "                # [bong][2023-04-28] 문장이 많은 경우에는 클러스터링 계수를 2,3배수로 함\n",
    "                emb = clustering_embedding(embeddings = embeddings, outmode=OUTMODE, num_clusters=(clustering_num*multiple), seed=SEED).astype(FLOAT_TYPE) \n",
    "            else:\n",
    "                emb = kmedoids_clustering_embedding(embeddings = embeddings, outmode=OUTMODE, num_clusters=(clustering_num*multiple), seed=SEED).astype(FLOAT_TYPE) \n",
    "            \n",
    "        # 1= 문장평균임베딩\n",
    "        elif EMBEDDING_METHOD == 1:\n",
    "            # 문장들에 대해 임베딩 값을 구하고 평균 구함.\n",
    "            arr = np.array(embeddings).astype(FLOAT_TYPE)\n",
    "            emb = arr.mean(axis=0).reshape(1,-1) #(128,) 배열을 (1,128) 형태로 만들기 위해 reshape 해줌\n",
    "            clustering_num = 1  # 평균값일때는 NUM_CLUSTERS=1로 해줌.\n",
    "        # 2=문장임베딩\n",
    "        else:\n",
    "            emb = embeddings\n",
    "\n",
    "        if i < 3:\n",
    "            myutils.log_message(f'*[index_data] cluster emb.shape: {emb.shape}', log_folder='../log/documet')\n",
    "            print()\n",
    "        \n",
    "        #--------------------------------------------------- \n",
    "        # docs에 저장 \n",
    "        #  [bong][2023-04-28] 여러개 벡터인 경우에는 벡터를 10개씩 분리해서 여러개 docs를 만듬.\n",
    "        for j in range(multiple):\n",
    "            count += 1\n",
    "            doc = {}                                #dict 선언\n",
    "            doc['rfile_name'] = rfile_names[i]      # contextid 담음\n",
    "            doc['rfile_text'] = rfile_texts[i]      # text 담음.\n",
    "            doc['dense_vectors'] = emb[j * clustering_num : (j+1) * clustering_num] # emb 담음.\n",
    "            docs.append(doc)\n",
    "        #---------------------------------------------------    \n",
    "\n",
    "            if count % BATCH_SIZE == 0:\n",
    "                mpower_index_batch(es, ES_INDEX_NAME, docs, vector_len=clustering_num, dim_size=dimension)\n",
    "                docs = []\n",
    "                myutils.log_message(\"[index_data](1) Indexed {} documents.\".format(count), log_folder='../log/documet')\n",
    "                 \n",
    "\n",
    "    if docs:\n",
    "        mpower_index_batch(es, ES_INDEX_NAME, docs, vector_len=clustering_num, dim_size=dimension)\n",
    "        myutils.log_message(\"[index_data](2) Indexed {} documents.\".format(count), log_folder='../log/documet')\n",
    "\n",
    "    es.indices.refresh(index=ES_INDEX_NAME)\n",
    "\n",
    "    myutils.log_message(f'*인덱싱 시간 : {time.time()-start:.4f}\\n', log_folder='../log/documet')\n",
    "    print()\n",
    "#---------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509df9d1-7b2e-48a4-a1e6-3c2751ed3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES 접속해서 임베딩 처리함.\n",
    "es = Elasticsearch(ES_URL)\n",
    "create_index(es, ES_INDEX_FILE, ES_INDEX_NAME, create=CREATE_INDEX)\n",
    "\n",
    "# 임베딩 모델 로딩\n",
    "WORD_EMBDDING_MODEL1, BI_ENCODER1 = bi_encoder(model_path=MODEL_PATH, max_seq_len=512, do_lower_case=True, \n",
    "                                               pooling_mode=POLLING_MODE, out_dimension=OUT_DIMENSION, device=DEVICE)\n",
    "\n",
    "print(BI_ENCODER1)\n",
    "print()\n",
    "try:\n",
    "    index_data(es, df_contexts, doc_sentences)\n",
    "except Exception as e:\n",
    "    error = f'index_data fail'\n",
    "    msg = f'{error}=>{e}'\n",
    "    myutils.log_message(f'/embed/es {msg}', log_folder='../log/documet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990059ea-bdc3-4328-9e0d-c2653cee6b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
